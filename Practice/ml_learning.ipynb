{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af87d2f6-7c5e-4a90-afa2-278535302ed8",
   "metadata": {},
   "source": [
    "# Machine Learning\r\n",
    "\r\n",
    "## Machine Learnign Types\r\n",
    "1. Supervised Learning :\r\n",
    "   1. Classification\r\n",
    "   2. Regres2ion\r\n",
    "3. Unsupervised Learning\r\n",
    "   1. Clustering\r\n",
    "   2. Dimensionality Reduction\r\n",
    "   3. Ass3ciation\r\n",
    "5. Reinforcement Learning\r\n",
    "\r\n",
    "\r\n",
    "### Supervised Learning :\r\n",
    "- Training Data and Labeled Data.\r\n",
    "- Both Input and Output.\r\n",
    "- Output should like having the relationship between input and output values.\r\n",
    "- Ex. `Training Data` ==learning algo==> `Model`\r\n",
    "    <br>New Input ==> `Model` ==> output\r\n",
    "- `Classification` is the task of predicting a category or label for a given input based on its feature.\r\n",
    "- `Regression` is the task of predicting a continuous value for a given input based on its feature.\r\n",
    "- Popular Supervised Algo:\r\n",
    "  1. Linear Regression (Regression algo)\r\n",
    "  2. Random Forest\r\n",
    "  3. Logistic Regression (Classification algo)\r\n",
    "  4. Support Vector Machines (SVM) (Classification and Regression algo)\r\n",
    "  5. Decision Tree\r\n",
    "  6. Gradient Boosting\r\n",
    "\r\n",
    "\r\n",
    "### Unsupervised Learning :\r\n",
    "==> Training a machine learning model on `unlabled data`  to identify patterns and relationships without prior knowledge of the outcome.\r\n",
    "- Only Inputs.\r\n",
    "\r\n",
    "- `Clustering` is the process of grouping similar data points together based on their features and characterstics.\r\n",
    "- The goal is to find patterns or structures.\r\n",
    "- Clustering Algo:\r\n",
    "  1. K-Means Clustering\r\n",
    "  2. Mean Shift Algo\r\n",
    "  3. DBSCAN Algo\r\n",
    "<br>\r\n",
    "- `Association`: identify patterns and relationships between varibales in data set. and finds which variables `occur together or independently`.\r\n",
    "- Association Algo:\r\n",
    "  1. H-Mine Algo\r\n",
    "  2. Apriori Algo\r\n",
    "  3. FP-Growth Algo\r\n",
    "<br>\r\n",
    "- `Dimensionality Reduction` : process of reducing the number of features or varibales in data set.\r\n",
    "- goal is to simplify the data and make it easier to analyze, visualize or process.\r\n",
    "- Dimensionality Reduction Algo :\r\n",
    "  1. Principal Component Analysis(PCA)\r\n",
    "  2. Autoencoders\r\n",
    "  3. Linear Discriminant Analysis(LDA)\r\n",
    "<br>\r\n",
    "- Popular `Unsupervised Learning` Algo:\r\n",
    "  1. KNN (K Nearest Neighbors)\r\n",
    "  2. K Means Clusting\r\n",
    "  3. Principal Component Analysis(PCA)\r\n",
    "  4. Apriori Algo\r\n",
    "  5. Hierarchical Clustering\r\n",
    "  6. Independent Component Analysis\r\n",
    "\r\n",
    "### Reinforcement Learning :\r\n",
    "==> Where agent learns to make decisions in an environment based on feedback in the form of rewards and punishments.\r\n",
    "- Also called `semi-supervised-learning`.\r\n",
    "- Works on reward and penalty.\r\n",
    "- Ex. `Agent` ==action==> `Environment`  ==state and reward==> `Agent`\r\n",
    "- `Reinforcement Learning` Algo:\r\n",
    "  1. Q-Learning\r\n",
    "  2. Markov Decision Process\r\n",
    "\r\n",
    "\r\n",
    "### Popular ML Algorithms:\r\n",
    "- Linear Regression\r\n",
    "- Logistic Regression\r\n",
    "- Traffic Prediction\r\n",
    "- Decision Tree\r\n",
    "- Random Forest\r\n",
    "- Support Vector Machines (SVM)\r\n",
    "- Naive Bayes\r\n",
    "- KNN (K Nearest Neighbors)\r\n",
    "- K Means Clusting\r\n",
    "\r\n",
    "\r\n",
    "## Terminology\r\n",
    "### 1. Overfitting :\r\n",
    "- Model performs well on training data, but does not in testing data.\r\n",
    "- In Overfitting => Cost function should be 0 or near to 0.\r\n",
    "- Solutions:\r\n",
    "    1. Gather more data\r\n",
    "    2. Reduce Noise\r\n",
    "    3. Use Regularization\r\n",
    "    4. Reduce some features\r\n",
    "\r\n",
    "\r\n",
    "### 2. Underfitting :\r\n",
    "- Model performs bad on training data,so it's obvious that it will perform bad in testing data.\r\n",
    "- In underfitting => Cost function is very high.\r\n",
    "- Solutions:\r\n",
    "    1. Gather more data\r\n",
    "    2. Reduce Noise\r\n",
    "\r\n",
    "\r\n",
    "### 3. Cost Function :\r\n",
    "- It is used for evaluating model.\r\n",
    "- Cost function is distance between predected values and actual values.\r\n",
    "- Higher the cost function will be the bad model and vice versa.\r\n",
    "- If all points on hypothisys => cost function will be 0.\r\n",
    "\r\n",
    "\r\n",
    "### 4. Regularization :\r\n",
    "- it is used to avoid `overfitting`.\r\n",
    "- It is achieved by `Lasso Regression` and `Ridge Regression`.\r\n",
    "\r\n",
    "\r\n",
    "### 5. Common Evaluation Metrics :\r\n",
    "- `MSE` = Mean Squared Error\r\n",
    "- `RMSE` = Root Mean Squared Error.\r\n",
    "    - Which is calculated by square root of MSE.\r\n",
    "- `r2_score` = R2 Score.\r\n",
    "    - 1.0 means good model.\r\n",
    "\r\n",
    "\r\n",
    "### 6. Principle Component Analysis (PCA):\r\n",
    "- PCA is used for `dimensionality reduction`.\r\n",
    "- When large dataset => Needs to dimensionality reduction.\r\n",
    "- PCA are unit vectors or new variables that are constructed as linear combinations of the initial variables.\r\n",
    "\r\n",
    "\r\n",
    "### 7. Data Standardization or normalization :\r\n",
    "- Steps :\r\n",
    "    1. Data Preprocessing\r\n",
    "    2. Compute covariance matrix\r\n",
    "    3. Compute Eigen vectors and values \r\n",
    "    4. Rearrange the Eigen vectors and eigen values\r\n",
    "    5. Compute cumulative energy content for each eigenvector\r\n",
    "    6. Select a subsety of Eigenvectors as basis vectors\r\n",
    "    7. Project the data onto new basis\r\n",
    "\r\n",
    "### 8. Learning Theory :\r\n",
    "- Bias and Variance :\r\n",
    "    - Generally `UnderFitting` models are `highly bias`. \r\n",
    "    - Generally `OverFitting` models are `highly variance`. \r\n",
    "    - Generally `good fit/robust` fit models are `low bias and low variance`. \r\n",
    "- Approx. Estimation Error\r\n",
    "- Empirical Risk Minimization 6. Select a subsety of Eigenvectors as basis vectors\r\n",
    "    7. Project the data onto new basis\r\n",
    "\r\n",
    "### 8. Learning Theory :\r\n",
    "- Bias and on will be lower the moral. and vice versa.\r\n",
    "- If all points on hypothisys => cost function will be 0.ion :\r\n",
    "- it is used to avoid `overfitting`.\r\n",
    "- It is achieved by `Lasso Regression` and `Ridge Regression`.on will be lower the moral. and vice versa.\r\n",
    "- If all points on hypothisys => cost function will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece725fa-4305-480c-a966-8ec852e58a62",
   "metadata": {},
   "source": [
    "## 1. Linear Regression :\r\n",
    "- dependent variables are known as responses\r\n",
    "- independent variables are known as features\r\n",
    "- Hypothesis function is calculated using features and responses.\r\n",
    "- “Y = mx + c” here, m is called as theta-1 and c is called as theta-0.\r\n",
    "- We can optimize theta by using gradient descent and Normal Equations Optimization algorithm.\r\n",
    "- If depedent and indepedent variable has multilinerality => use `polylinear regression`.\r\n",
    "\r\n",
    "\r\n",
    "## 2. Logistic Regression :\r\n",
    "- it is a `Classification` algorithm.\r\n",
    "- added `sigmoid` for getting results between 0 and 1.\r\n",
    "- result value 1 indicates 100% prediction on positive side.\r\n",
    "- if result value 0 then indicates 100% prediction on negative side.\r\n",
    "- if result 0.5 then indicates model can't predict on positive or negative side.\r\n",
    "\r\n",
    "\r\n",
    "## 3. Support Vector Machine :\r\n",
    "- It is `Classification` and `Regression` type algorithm.\r\n",
    "- It creates a `hypterplane` and 2 parallel support vectors on both side of hyperplane.\r\n",
    "- Distance between hyperplane and one of the parallel support vectors is called `margin`.\r\n",
    "- SVM try to set maximum margin on both side.\r\n",
    "- Try to set max margin in such a way that the nearest point is far away from the hyperplane.\r\n",
    "- `Hard Margin Classification` and `Soft Margin Classification`.\r\n",
    "- Hard Margin Classification is strictly avoid data points between hyperplane and support vector.\r\n",
    "- Support Vector is created such a way that some points may be between the support vector and hyperplane when using Soft Margin Classification.\r\n",
    "- When there is non-linearality => Use `Kernal Trick`.\r\n",
    "- In kernal trick => Makes 1-D data to high dimensional data.\r\n",
    "\r\n",
    "to high dimensional data.\r\n",
    "\r\n",
    "rgin Classification`.\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
